<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TensorTouch</title>
  <link rel="icon" href="./data/favicon.png" type="image/png">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <style>
    :root {
      --accent: #3478f6;
      --bg: #ffffff;
      --text: #111;
      --sidebar-bg: #f8f9fa;
      --sidebar-text: #555;
      --sidebar-active: var(--accent);
      --title-font: 'Inter', system-ui, sans-serif;
      --body-font: 'Inter', system-ui, sans-serif;
      --sidebar-width: max(20vw, 180px);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: var(--body-font);
      background-color: var(--bg);
      color: var(--text);
      display: flex;
      scroll-behavior: smooth;
      font-size: 1.08rem;
      line-height: 1.6;
      /* background: linear-gradient(135deg, #ffffff 0%, #8fbae0 100%); */
    }
    html {
      background: linear-gradient(135deg, #ffffff 0%, #0075d5 100%);
      min-height: 100vh;
    }

    .sidebar {
      width: var(--sidebar-width);
      position: fixed;
      height: 100vh;
      overflow-y: auto;
      background: #fff;
      border: none;
      padding: 2rem 1rem;
      text-align: right;
    }

    .sidebar h2 {
      font-size: 1.25rem;
      margin-bottom: 1rem;
      color: var(--text);
    }

    .sidebar ul {
      list-style: none;
      padding: 0;
    }

    .sidebar ul li {
      margin-bottom: 0.75rem;
    }

    .sidebar a {
      text-decoration: none;
      color: var(--sidebar-text);
      font-weight: 500;
      transition: color 0.25s, font-weight 0.25s, text-shadow 0.25s;
    }

    .sidebar a.active,
    .sidebar a:hover {
      color: var(--sidebar-active);
      font-weight: 700;
      text-shadow: 0 2px 8px rgba(52, 120, 246, 0.10), 0 1px 0 #fff;
      text-decoration: none;
    }

    main {
      /* margin-left: var(--sidebar-width); */
      margin: auto;
      width: calc(80% - var(--sidebar-width));
      max-width: 1000px;
      padding: 0 2rem;
      box-sizing: border-box;
    }

    section {
      margin-bottom: 3rem;
    }

    h1,
    h2 {
      color: #1a1a1a;
      font-family: var(--title-font);
      font-weight: 700;
      letter-spacing: -0.5px;
    }

    h1 {
      font-size: 2.1rem;
      margin-bottom: 0.5rem;
    }

    h2 {
      font-size: 1.35rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }

    .main-title {
      font-size: 4rem;
      font-weight: 800;
      font-family: var(--title-font);
      margin-bottom: 0.2rem;
      line-height: 1.1;
      letter-spacing: -1px;
      text-align: left;
      color: #111;
    }

    .main-title .gradient {
      background: linear-gradient(90deg, #86abf0 0%, #0052ea 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      color: transparent;
      font-weight: 900;
    }

    .subtitle {
      font-size: 1.95rem;
      font-weight: 500;
      color: #222;
      margin-bottom: 1.1rem;
      line-height: 1.3;
      text-align: left;
    }

    .video-carousel {
      display: flex;
      overflow-x: auto;
      gap: 0.75rem;
      padding: 0.5rem 4px;
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      box-sizing: border-box;
      white-space: nowrap;
    }

    .video-carousel video {
      width: 240px;
      border-radius: 4px;
      border: 1px solid #ccc;
      flex-shrink: 0;
    }

    .iframe-pair {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      margin-top: 1rem;
      justify-content: center;
    }

    .iframe-pair iframe {
      flex: 1 1 48%;
      height: 400px;
      border: 1px solid #ccc;
      border-radius: 4px;
    }

    img {
      max-width: 100%;
      border-radius: 4px;
      /* border: 1px solid #ccc; */
    }

    pre {
      background: #f0f0f0;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.92em;
    }

    code {
      font-size: 0.92em;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 1rem;
    }

    @media (max-width: 900px) {
      .sidebar {
        display: none;
      }

      :root {
        --sidebar-width: 0;
      }

      main {
        margin-left: 0;
        width: 100%;
        padding: 0;
      }

      .main-content {
        max-width: 100vw;
        padding: 0 8px;
      }

      .video-carousel {
        max-width: 100vw;
        padding: 0.5rem 8px;
      }
    }

    .main-content {
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      overflow: visible;
    }
  </style>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const links = document.querySelectorAll('.sidebar a');
      const sections = Array.from(document.querySelectorAll('main section'));

      function updateActiveLink() {
        const scrollY = window.scrollY + 100;
        let current = sections[0];

        for (const section of sections) {
          if (section.offsetTop <= scrollY) {
            current = section;
          }
        }

        links.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === `#${current.id}`) {
            link.classList.add('active');
          }
        });
      }

      window.addEventListener('scroll', updateActiveLink);
      updateActiveLink();

      // Iframe carousel logic
      const iframeCarousel = document.getElementById('iframe-carousel');
      const iframeLabel = document.getElementById('iframe-carousel-label');
      const prevBtn = document.getElementById('iframe-carousel-prev');
      const nextBtn = document.getElementById('iframe-carousel-next');
      const iframeData = [
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/tiger_pick_r2r2r_recording_20250507_112639.viser&initialCameraPosition=1.027,0.445,0.681&initialCameraLookAt=0.188,-0.113,-0.129&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Tiger Pick'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/faucet_recording_20250507_110111.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Faucet Turn'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/package_recording_20250507_103622.viser&initialCameraPosition=1.050,0.428,0.670&initialCameraLookAt=0.000,0.000,0.000&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Package Open'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/drawer_reversed_demo.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Drawer Pull'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/mug_demo.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Mug Place'
        }
      ];
      let iframeIdx = 0;
      function updateIframeCarousel() {
        iframeCarousel.src = iframeData[iframeIdx].src;
        iframeLabel.textContent = iframeData[iframeIdx].label;
      }
      if (prevBtn && nextBtn && iframeCarousel && iframeLabel) {
        prevBtn.addEventListener('click', () => {
          iframeIdx = (iframeIdx - 1 + iframeData.length) % iframeData.length;
          updateIframeCarousel();
        });
        nextBtn.addEventListener('click', () => {
          iframeIdx = (iframeIdx + 1) % iframeData.length;
          updateIframeCarousel();
        });
      }

      // Main iframe swap logic for thumbnails
      const mainIframe = document.getElementById('main-iframe');
      const thumbnailVideos = document.querySelectorAll('#iframe-thumbnails video');
      const iframeSrcs = [
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/faucet_recording_20250507_110111.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/cardboard_box_recording_20250507_103622.viser&initialCameraPosition=1.050,0.428,0.670&initialCameraLookAt=0.000,0.000,0.000&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/drawer_recording_20250507_104056.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/replace_this_later.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/tiger_pick_r2r2r_recording_20250507_112639.viser&initialCameraPosition=1.027,0.445,0.681&initialCameraLookAt=0.188,-0.113,-0.129&initialCameraUp=-0.000,-0.000,1.000',
      ];
      if (mainIframe && thumbnailVideos.length === iframeSrcs.length) {
        thumbnailVideos.forEach((vid, idx) => {
          vid.addEventListener('click', () => {
            mainIframe.src = iframeSrcs[idx];
            // Highlight the selected video
            thumbnailVideos.forEach(v => v.style.outline = '');
            vid.style.outline = '3px solid var(--accent)';
          });
        });
        // Highlight the first by default
        thumbnailVideos[0].style.outline = '3px solid var(--accent)';
      }

      // Physical rollouts main video selector logic
      const mainPhysicalVideo = document.getElementById('main-physical-video');
      const physicalThumbnails = document.querySelectorAll('#physical-video-thumbnails video');
      if (mainPhysicalVideo && physicalThumbnails.length > 0) {
        physicalThumbnails.forEach((vid, idx) => {
          vid.addEventListener('click', () => {
            const src = vid.getAttribute('data-video');
            if (src) {
              mainPhysicalVideo.querySelector('source').src = src;
              mainPhysicalVideo.load();
              mainPhysicalVideo.play();
              // Highlight selected
              physicalThumbnails.forEach(v => v.style.outline = '');
              vid.style.outline = '3px solid var(--accent)';
            }
          });
        });
        // Highlight the first by default
        physicalThumbnails[0].style.outline = '3px solid var(--accent)';
      }
    });
  </script>
</head>

<body>
  <nav class="sidebar">

    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#realrollouts">Real World Tactile Demos</a></li>
      <li><a href="#method-at-a-glance">Method at a Glance</a></li>
      <li><a href="#data-collection">Data Collection</a></li>
      <li><a href="#fe-simulation">FE Simulation Pipeline</a></li>
      <li><a href="#tensor">Construction of Deformation and Stress Tensors</a></li>
      <li><a href="#model">Model</a></li>
      <li><a href="#results">Results</a></li>
    </ul>
  </nav>
  <main>
    <div class="main-content">
      <section id="title-block">
        <div style="margin-top: 20px;"></div>
        <div class="main-title"><span>Tensor</span><span class="gradient">Touch</span></div>
        <div class="subtitle">Calibration of Tactile Sensors for High Resolution
          Stress Tensor and Deformation for Dexterous Manipulation</div>
        <p style="font-size: 1.1rem; margin-bottom: 0.1rem;">Won-Kyung Do, Matthew Strong, Aiden Swann, Boshu Lei,
          Monroe Kennedy III</p>
      </section>

      <section id="overview-video">
        <div style="display: flex; justify-content: center; width: 100%; margin: 1rem 0;">
          <video autoplay muted loop playsinline
            style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
            <source src="data/twitter-final-compress.mov" type="video/mp4">
          </video>
        </div>
        <div style="max-width: 900px; margin: 0 auto 1.5rem auto; font-size: 1.15rem; color: #222; text-align: center;">
          <b>Real2Render2Real (R2R2R)</b> is a scalable pipeline for generating data to train generalist manipulation
          policies - without physics simulation or teleoperation.
        </div>
        <div style="text-align: center; margin-bottom: 2.5rem;">
          <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[Paper
            (coming soon)]</a>
          <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[arXiv
            (coming soon)]</a>
          <a href="#" style="color: #3478f6; text-decoration: none; font-weight: 600;">[Code (coming soon)]</a>
        </div>
      </section>
      <section id="abstract">
        <h1>Abstract</h1>
        Advanced dexterous manipulation involving multiple simultaneous contacts across different surfaces, like
        pinching coins from ground or manipulating intertwined objects, remains challenging for robotic systems. Such
        tasks exceed the capabilities of vision and proprioception alone, requiring high-resolution tactile sensing with
        calibrated physical metrics. Raw optical tactile sensor images, while information-rich, lack interpretability
        and cross-sensor transferability, limiting their utility in real-world applications. TensorTouch addresses this
        challenge by integrating finite element analysis with deep learning to extract comprehensive contact information
        from optical tactile sensors, including stress tensors, deformation fields, and force distributions at
        pixel-level resolution. Our framework achieves sub-millimeter position accuracy and precise force estimation
        while supporting large sensor deformations, crucial for manipulating soft or delicate objects. Experimental
        validation demonstrates the effectiveness of our method in a challenging multi-object manipulation task of
        selectively grasping one of two strings based on detected motion- <b>achieving 90% success with objects of
          different material properties. </b> This enables new capabilities in contact-rich manipulation scenarios
        previously inaccessible to robotic systems.</p>
      </section>

      <section id="realrollouts">
        <h2>Real World Tactile Demos</h2>
        <div style="display: flex; justify-content: center; width: 100%; margin-bottom: 1rem;">
          <video id="main-physical-video" autoplay muted loop playsinline
            style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
            <source src="data/real_vids/faucet_real_5x_side.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-carousel" id="physical-video-thumbnails">
          <video data-video="data/real_vids/faucet_real_5x_side.mp4" autoplay muted loop playsinline
            src="data/real_vids/faucet_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/mug_real_5x_side.mp4" autoplay muted loop playsinline
            src="data/real_vids/mug_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/franka_real_30x_front.mp4" autoplay muted loop playsinline
            src="data/real_vids/franka_real_30x_front.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/package_real_5x_side.mp4" autoplay muted loop playsinline
            src="data/real_vids/package_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/drawer_real_5x_side.mp4" autoplay muted loop playsinline
            src="data/real_vids/drawer_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/tiger_real_5x_side.mp4" autoplay muted loop playsinline
            src="data/real_vids/tiger_real_5x_side.mp4" style="cursor:pointer;"></video>
        </div>
      </section>

      <section id="method-at-a-glance">
        <h2>Method at a Glance</h2>
        <div style="display: flex; justify-content: center; width: 100%; margin: 2rem 0;">
          <img src="data/r2r2r_plot.png" alt="Performance Plot"
            style="max-width: 900px; width: 100%; border-radius: 4px">
        </div>
        <p>Comparative analysis of imitation-learning policies trained on R2R2R-generated data against human
          teleoperation data across 1050 physical robot experiments and 5 robotic tasks.</p>
      </section>

      <section id="data-collection">
        <h2>Data Collection</h2>
        <!-- Main Iframe and Video Thumbnails as Interactive Selector -->
        <div style="display: flex; flex-direction: column; align-items: center; width: 100%;">
          <iframe id="main-iframe"
            src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/tiger_pick_r2r2r_recording_20250507_112639.viser&initialCameraPosition=1.027,0.445,0.681&initialCameraLookAt=0.188,-0.113,-0.129&initialCameraUp=-0.000,-0.000,1.000"
            style="width: 100%; max-width: 100%; height: 460px; border: 1px solid #ccc; border-radius: 8px; margin-bottom: 0.75rem; box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);"></iframe>
          <div class="video-carousel" id="iframe-thumbnails">
            <video data-iframe="0" autoplay muted loop playsinline src="data/demo_vids/faucet_demo.mp4"
              style="cursor:pointer;"></video>
            <video data-iframe="1" autoplay muted loop playsinline src="data/demo_vids/package_demo.mp4"
              style="cursor:pointer;"></video>
            <video data-iframe="2" autoplay muted loop playsinline src="data/demo_vids/drawer_reversed_demo.mp4"
              style="cursor:pointer;"></video>
            <video data-iframe="3" autoplay muted loop playsinline src="data/demo_vids/mug_demo.mp4"
              style="cursor:pointer;"></video>
            <video data-iframe="4" autoplay muted loop playsinline src="data/demo_vids/tiger_demo.mp4"
              style="cursor:pointer;"></video>
          </div>
        </div>
        <!-- <div style="display: flex; gap: 1rem; width: 100%; margin-top: 1rem;">
        <iframe src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/faucet_recording_20250507_110111.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000" style="flex: 1 1 48%; height: 360px; border: 1px solid #ccc; border-radius: 8px;"></iframe>
        <iframe src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/drawer_reversed_demo_pair.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000" style="flex: 1 1 48%; height: 360px; border: 1px solid #ccc; border-radius: 8px;"></iframe>
      </div> -->
      </section>

      <section id="The Distinction Between Simulation and Rendering">
        <h3>This is often a point of confusion for our project so here's a section dedicated to clearing it up:</h3>
        <p> When we refer to <b>simulation</b>, we mean the use of a physics engine to computationally model and
          simulate dynamic interactions in the environment. In contrast, <b>rendering</b> refers to generating visual
          data from a graphics engine. In our project, we use IsaacLab, which is commonly used as a physics simulator.
          However, we disable its dynamics simulation capabilities and instead use it solely as a rendering engine. This
          allows us to generate high-quality synthetic visual observations and robot kinematic data, which can be used
          to train imitation learning policies.</p>
        <table>
          <tr>

          </tr>
        </table>
      </section>

      <section id="fe-simulation">
        <h2>Finite Element Simulation</h2>
        <p>Part trajectories from a single demonstration can be retargeted across different robot embodiments.</p>
        <div class="iframe-pair">
          <iframe
            src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/coffee_maker_recording_20250506_191701.viser&initialCameraPosition=1.009,0.430,0.637&initialCameraLookAt=0.240,-0.160,-0.081&initialCameraUp=-0.000,-0.000,1.000"
            width="480" height="360"></iframe>
          <iframe
            src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/franka_coffee_maker_recording_20250506_210619.viser&initialCameraPosition=0.991,-0.089,0.591&initialCameraLookAt=0.000,0.000,0.000&initialCameraUp=-0.000,-0.000,1.000"
            width="480" height="360"></iframe>
        </div>
        <div class="iframe-pair">
          <video src="data/real_vids/mug_real_5x_side.mp4" autoplay muted loop playsinline
            style="max-width: 450px;border-radius: 5px;"></video>
          <video src="data/real_vids/franka_real_30x_front.mp4" autoplay muted loop playsinline
            style="max-width: 450px;border-radius: 5px;"></video>
        </div>
      </section>

      <section id="tensor">
        <h2>Construction of Deformation and Stress Tensor</h2>
        <p>We randomize initial object poses to generate diverse synthetic rollouts for each object-task combination.
        </p>
        <div class="video-carousel">
          <video autoplay muted loop playsinline src="data/randomization/faucet_rand_range.mp4"></video>
          <video autoplay muted loop playsinline src="data/randomization/package_rand_range.mp4"></video>
          <video autoplay muted loop playsinline src="data/randomization/drawer_rand_range.mp4"></video>
          <video autoplay muted loop playsinline src="data/randomization/mug_rand_range.mp4"></video>
          <video autoplay muted loop playsinline src="data/randomization/tiger_rand_range.mp4"></video>
          <video autoplay muted loop playsinline src="data/randomization/franka_rand_range.mp4"></video>
        </div>
      </section>

      <section id="model">
        <h2>Tactile Model</h2>
        <p>From a single demonstration, R2R2R generates a distribution of plausible trajectories by interpolating 6-DoF
          part motion.</p>
        <img src="data/traj_interp.png" alt="Trajectory Interpolation" style="border: 1px solid #ccc;">
      </section>

      <section id="results">
        <h2>Results</h2>
        <p>From a single demonstration, R2R2R generates a distribution of plausible trajectories by interpolating 6-DoF
          part motion.</p>
        <img src="data/traj_interp.png" alt="Trajectory Interpolation" style="border: 1px solid #ccc;">
      </section>

      <section id="testing">
        <h2>Placeholder</h2>
        <p>From a single demonstration, R2R2R generates a distribution of plausible trajectories by interpolating 6-DoF
          part motion.</p>
        <img src="data/traj_interp.png" alt="Trajectory Interpolation" style="border: 1px solid #ccc;">
      </section>
      <!-- 
    <section id="bibtex">
      <h2>BibTeX (To be Updated)</h2>
      <pre><code>@article{real2render2real,
  title={Real2Render2Real: Scaling Robotic Manipulation Data Without Dynamics Simulation or Robot Hardware},
  author={real2render2real@gmail.com},
  year={2025},
}</code></pre>
    </section> -->
      <footer>
        &copy; Webpage heavily inspired by the TensorTouch team, inspired by the <a
          href="https://www.videomimic.net/">VideoMimic</a> website.
      </footer>
    </div>
  </main>
</body>

</html>